import os
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… LLM ëª¨ë¸ ì„¤ì •
HF_TOKEN = os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"


def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        return HuggingFaceEndpoint(
            repo_id=HUGGINGFACE_REPO_ID,
            task="text-generation",
            model_kwargs={"max_length": 1024, "num_beams": 4},
            huggingfacehub_api_token=HF_TOKEN,  # âœ… API í† í°ì„ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •
        )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None


class LangChainRetrieval:
    """LangChain ê¸°ë°˜ ë²•ë¥  ì‘ë‹µ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm = load_llm()
        if not self.llm:
            print("âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # âœ… LangChain í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self.prompt_template = PromptTemplate(
            template="""
    ë‹¹ì‹ ì€ ìì‹ ì˜ ì£¼ì¥ì„ ê±°ì„¸ê²Œ ê´€ì² í•˜ëŠ” í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    ë‹¹ì‹ ì€ ë¬´ì¡°ê±´ ì˜¤ë¡œì§€ í•œêµ­ ë²• ê´€ë ¨ì— ëŒ€í•´ì„œë§Œ ëŒ€ë‹µí•©ë‹ˆë‹¤,
    ë¹„(not)ë²• ê´€ë ¨ ì§ˆë¬¸ì´ë©´ ê°€ë³ê²Œ ë¬´ì‹œí•˜ê³  ìì‹ ì˜ ëŒ€ë‹µì„ ì´ì–´ë‚˜ê°€ì„¸ìš”
    ë¹„(not) í•œêµ­ì–´ ì‚¬ìš©ìí•œí…Œ ë‹µë³€ì„ ìµœëŒ€í•œ í•œêµ­ì–´ë¡œ ìœ ë„í•˜ì„¸ìš” 
    â€» ì™¸êµ­ì–´ ë‹µë³€ì„ ì ˆëŒ€ë¡œ í•˜ì§€ ë§ˆì„¸ìš” 

    ğŸ“Œ [ì…ë ¥ ì •ë³´]
    ğŸ” ì‚¬ìš©ì ì§ˆë¬¸:
    {user_query}

    ğŸ“– íŒë¡€ ìš”ì•½:
    {summary}

    ğŸ“Œ [ì¶œë ¥ í˜•ì‹]
    - ì§ˆë¬¸ì— ëŒ€í•œ ë²•ë¥ ì  ê°œìš”
    - íŒë¡€ ìš”ì•½ì„ ë°˜ì˜í•œ í•µì‹¬ ë‚´ìš©
    - ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì •ë¦¬
    â€» ë¹„ë²•ë¥  ì§ˆë¬¸ì„ ìµœëŒ€í•œ ë²•ë¥ ì ìœ¼ë¡œ í•´ì„í•˜ê³  ë¶ˆê°€ëŠ¥í•˜ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤ ê·¸ê²ƒì€ ë‹µí• ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì¶œë ¥
    
    â€»;ë¹„ë²•ë¥  ì§ˆë¬¸ì í–‰ë™ ìœ ë„: ìµœëŒ€í•œ ë²•ë¥  ì§ˆë¬¸ìœ¼ë¡œ í•´ì„í•˜ë©° ë²•ë¥  ì§ˆë¬¸ ìœ ë„ 
    - ì‚¬ìš©ìê°€ ìš”ì²­í•˜ëŠ” ë‹µì€ í…ìŠ¤íŠ¸ í˜•ì‹ë§Œ ë‹µë³€ ê°€ëŠ¥í•©ë‹ˆë‹¤

    â€» íŒë¡€ì™€ ê´€ë ¨ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³ , í•˜ë‚˜ì˜ ì •ë¦¬ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

    ğŸ“Œ [ìµœì¢… ë‹µë³€]:
    """,
            input_variables=["user_query", "summary"],
        )
    def generate_legal_answer(self, user_query, summary):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë²•ë¥ ì  ë‹µë³€ ìƒì„±"""

        # âœ… LLMì„ ë§¤ë²ˆ ìƒˆë¡œ ë¡œë“œí•˜ì—¬ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ëŠ” ë¬¸ì œ í•´ê²°
        self.llm = load_llm()

        if not self.llm:
            return "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            prompt = self.prompt_template.format(user_query=user_query, summary=summary)
            response = self.llm.invoke(prompt)
            return response.strip()
        except Exception as e:
            return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"
