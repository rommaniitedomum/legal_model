import os
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… LLM ëª¨ë¸ ì„¤ì •
HF_TOKEN = os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"


def load_llm():
    """LLM ë¡œë“œ (HuggingFace Inference API)"""
    try:
        return HuggingFaceEndpoint(
            repo_id="mistralai/Mistral-7B-Instruct-v0.3",
            task="text-generation",
            temperature=0.3,  # âœ… ëœë¤ì„± ì¤„ì´ê³  ì¼ê´€ì„± ë†’ì´ê¸°
            top_p=0.85,  # âœ… ëª¨ë¸ì´ ì¢€ ë” ì •ë°€í•œ ì¶œë ¥ì„ í•˜ë„ë¡ ì¡°ì •
            model_kwargs={
                "max_length": 1024,  # âœ… ì¶©ë¶„í•œ ê¸¸ì´ ë³´ì¥
                "num_beams": 2,  # âœ… ë„ˆë¬´ ë†’ì€ beam search ë°©ì§€
            },
            huggingfacehub_api_token=HF_TOKEN,
        )
    except Exception as e:
        print(f"âŒ [LLM ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None



class LangChainRetrieval:
    """LangChain ê¸°ë°˜ ë²•ë¥  ì‘ë‹µ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        self.llm = load_llm()
        if not self.llm:
            print("âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # âœ… LangChain í”„ë¡¬í”„íŠ¸ ì„¤ì • (ìµœì í™”)
        self.prompt_template = PromptTemplate(
            template="""
    ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    ë²•ë¥ ê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ì§ˆë¬¸ì€ "ì£„ì†¡í•©ë‹ˆë‹¤, ë²•ë¥ ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    ğŸ“Œ [ì…ë ¥ ì •ë³´]
    ğŸ” ì‚¬ìš©ì ì§ˆë¬¸:
    {user_query}

    ğŸ“– íŒë¡€ ìš”ì•½:
    {summary}

    ğŸ“Œ [ì¶œë ¥ í˜•ì‹]
    - ì§ˆë¬¸ì— ëŒ€í•œ ë²•ë¥ ì  ê°œìš”
    - íŒë¡€ ìš”ì•½ì„ ë°˜ì˜í•œ í•µì‹¬ ë‚´ìš©
    - ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì •ë¦¬
    - **ë¹„ì •ìƒì ì¸ ë‹¨ì–´, ë¬´ì˜ë¯¸í•œ ì¶œë ¥, ì™¸êµ­ì–´ í˜¼í•© ì‚¬ìš©ì„ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.**
    
    ğŸ“Œ [ìµœì¢… ë‹µë³€]:  
    """,
            input_variables=["user_query", "summary"],
        )

    def generate_legal_answer(self, user_query, summary):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë²•ë¥ ì  ë‹µë³€ ìƒì„±"""

        # âœ… LLMì„ ë§¤ë²ˆ ìƒˆë¡œ ë¡œë“œí•˜ì—¬ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ëŠ” ë¬¸ì œ í•´ê²°
        self.llm = load_llm()

        if not self.llm:
            return "âŒ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            prompt = self.prompt_template.format(user_query=user_query, summary=summary)
            response = self.llm.invoke(prompt).strip()

            # âœ… ì‘ë‹µì´ ë¹„ì •ìƒì ì¸ ê²½ìš°, ì¬ìš”ì²­ (1íšŒ)
            if not response or len(response) < 10 in response:
                print("âš ï¸ [ê²½ê³ ] ë¹„ì •ìƒì ì¸ ì‘ë‹µ ê°ì§€ â†’ LLM ì¬ìš”ì²­")
                response = self.llm.invoke(prompt).strip()

            # âœ… ì—¬ì „íˆ ë¹„ì •ìƒì ì¸ ê²½ìš°, ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
            return response if response else "âŒ ì •ìƒì ì¸ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"âŒ LLM ì˜¤ë¥˜: {str(e)}"
