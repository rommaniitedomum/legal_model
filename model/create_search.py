import os
import torch
import re
from transformers import (
    BartForConditionalGeneration,
    AutoTokenizer,
    BertForSequenceClassification,
    AutoConfig,
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from safetensors.torch import load_file
from dotenv import load_dotenv
from kobart import get_pytorch_kobart_model, get_kobart_tokenizer

from langchain_retriever import LangChainRetrieval

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… FAISS ë²¡í„°DB ë¡œë“œ
DB_FAISS_PATH = "vectorstore/db_faiss"


def load_faiss():
    """FAISS ë¡œë“œ"""
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        return FAISS.load_local(
            DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True
        )
    except Exception as e:
        print(f"âŒ [FAISS ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None

langchain_retriever = LangChainRetrieval()


# âœ… BART íŒë¡€ ìš”ì•½ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "./model/1.íŒë¡€ìš”ì•½ëª¨ë¸/checkpoint-26606"
def load_bart():
    """KoBART ëª¨ë¸ ë¡œë“œ"""
    try:
        print("ğŸ” KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # âœ… KoBART ëª¨ë¸ ë¡œë“œ
        model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())

        # âœ… KoBART ì „ìš© í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = get_kobart_tokenizer()
        tokenizer.pad_token_id = tokenizer.eos_token_id
        tokenizer.model_max_length = 1024
        
        # âœ… `model.safetensors`ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
        state_dict = load_file(os.path.join(MODEL_PATH, "model.safetensors"))

        # âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì ìš©
        model.load_state_dict(state_dict, strict=False)
        # ì£¼ì˜ì‚¬í•­ ëˆ„ë½ëœ í‚¤ê°€ ì¡´ì¬ í•˜ì§€ë§Œ ëª¨ë¸ì— ì§ì ‘ì ì¸ ì„±ëŠ¥ì„ ì£¼ì§€ ì•Šê¸°ì— ë¬´ì‹œ
        # ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ 
            
        # âœ… ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì „í™˜
        model.eval()
        print("âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return tokenizer, model

    except Exception as e:
        print(f"âŒ [KoBART ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None, None

def summarize_case(text, tokenizer, model):
    """íŒë¡€ ìš”ì•½: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì•¼ ìš”ì•½ì„ ìˆ˜í–‰í•˜ë„ë¡ í•¨"""
    try:
        char_length = len(text)
        word_count = len(text.split())
        print(
            f"ğŸ” [DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ììˆ˜): {char_length}, ë‹¨ì–´ ìˆ˜: {word_count}"
        )

        if word_count < 5 or char_length < 20:
            return "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ìš”ì•½ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # âœ… í† í°í™” í›„ ì¸ì½”ë”©ëœ ê°’ í™•ì¸
        input_ids = tokenizer.encode(
            text, return_tensors="pt", max_length=1024, truncation=True
        )
        print(f"ğŸ” [DEBUG] BART input_ids: {input_ids}")

        # âœ… ëª¨ë¸ì˜ vocab_size ë²”ìœ„ ë‚´ë¡œ ê°’ ì œí•œ
        input_ids = torch.clamp(input_ids, min=0, max=model.config.vocab_size - 1)

        print("ğŸš€ [INFO] `generate()` ì‹¤í–‰ ì‹œì‘")
        # summary_ids = model.generate(
        #     input_ids,
        #     max_length=512,
        #     min_length=120,
        #     num_beams=8,
        #     early_stopping=True,
        #     no_repeat_ngram_size=3,
        #     repetition_penalty=2.5,
        #     length_penalty=1.0,
        # ) # ëª¨ë¸ 1 (ì •í™•ë„)
        summary_ids = model.generate(
            input_ids,
            max_length=200,
            min_length=120,
            num_beams=8,
            early_stopping=True,
            no_repeat_ngram_size=3,
            repetition_penalty=1.8,
            length_penalty=1,
        )    # ëª¨ë¸ 2 (ì†ë„ + ì •í™•) ì‚¬ìš© 
        # summary_ids = model.generate(
        #     input_ids,
        #     max_length=150,  # âœ… ì‘ë‹µ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì§§ê²Œ ì„¤ì • (200 â†’ 150)
        #     min_length=120,  # âœ… ìµœì†Œí•œì˜ ì •ë³´ í¬í•¨ (80~120 ìœ ì§€)
        #     num_beams=4,  # âœ… beams ê°ì†Œë¡œ ì†ë„ ì¦ê°€ (8 â†’ 4)
        #     early_stopping=True,
        #     no_repeat_ngram_size=3,
        #     repetition_penalty=1.5,  # âœ… ë°˜ë³µ ìµœì†Œí™” (2.0 â†’ 1.5)
        #     length_penalty=0.8,  # âœ… ë” ì§§ì€ ìš”ì•½ ìƒì„± (1.0 â†’ 0.8)
        # ) # ëª¨ë¸ 3 ì†ë„ë§Œ ë¹ ë¥´ê³  ë¶€ì •í™• 
        print(f"ğŸ” [DEBUG] summary_ids: {summary_ids}")

        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print(f"ğŸ” [DEBUG] ìš”ì•½ ê²°ê³¼: {decoded}")

        return decoded

    except Exception as e:
        print(f"âŒ [íŒë¡€ ìš”ì•½ ì˜¤ë¥˜] {e}")
        return "âŒ ìš”ì•½ ì‹¤íŒ¨"


# âœ… BERT íŒê²° ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
JUDGMENT_MODEL_PATH = "./model/2.íŒê²°ì˜ˆì¸¡ëª¨ë¸/20240222_best_bert.pth"


def load_bert():
    """BERT íŒê²° ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ"""
    try:
        print("ğŸ” BERT ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        print("âœ… BERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")

        # âœ… ì„¤ì •ê°’ ë¶ˆëŸ¬ì˜¤ê¸°
        config = AutoConfig.from_pretrained("bert-base-uncased")
        config.num_labels = 3  # 0: ë¬´ì£„, 1: ìœ ì£„, 2: ë¶ˆëª…í™•
        config.id2label = {0: "ë¬´ì£„", 1: "ìœ ì£„", 2: "ë¶ˆëª…í™•"}
        config.label2id = {"ë¬´ì£„": 0, "ìœ ì£„": 1, "ë¶ˆëª…í™•": 2}

        # âœ… BERT ëª¨ë¸ ì´ˆê¸°í™”
        model = BertForSequenceClassification.from_pretrained(
            "bert-base-uncased", config=config
        )
        print("âœ… BERT ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")

        # âœ… ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì ìš© (map_location="cpu"ë¡œ CPU ë¡œë“œ)
        state_dict = torch.load(JUDGMENT_MODEL_PATH, map_location="cpu")
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

        # âœ… ê°€ì¤‘ì¹˜ ì²´í¬
        if missing_keys:
            print(f"âŒ [ê²½ê³ ] ëª¨ë¸ ê°€ì¤‘ì¹˜ ëˆ„ë½: {missing_keys}")
        if unexpected_keys:
            print(f"âš  [ê²½ê³ ] ì˜ˆìƒì¹˜ ëª»í•œ ê°€ì¤‘ì¹˜: {unexpected_keys}")

        # âœ… ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        print("âœ… BERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ")

        return tokenizer, model

    except Exception as e:
        print(f"âŒ [BERT ë¡œë“œ ì˜¤ë¥˜] {e}")
        return None, None


def predict_judgment(text, tokenizer, model):
    """íŒê²° ì˜ˆì¸¡"""
    try:
        # inputs = tokenizer(
        #     text,
        #     return_tensors="pt",
        #     max_length=512,
        #     truncation=True,
        #     padding="max_length",
        # ) # ê¸°ë³¸ì„¤ì • 
        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=300,  # âœ… ë¶ˆí•„ìš”í•œ ì—°ì‚° ì¤„ì´ê¸° ìœ„í•´ 300ìœ¼ë¡œ ì„¤ì •
            truncation=True,
            padding="longest",  # âœ… ë¶ˆí•„ìš”í•œ íŒ¨ë”© ìµœì†Œí™”
        ) # 2ì°¨ ì¡°ì • 
        
        inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])

        with torch.no_grad():
            logits = model(**inputs).logits
            print(f"ğŸ” [DEBUG] BERT logits: {logits}")
            probabilities = torch.nn.functional.softmax(logits, dim=1)


        return probabilities.tolist()

    except Exception as e:
        print(f"âŒ [íŒê²° ì˜ˆì¸¡ ì˜¤ë¥˜] {e}")
        return "âŒ ì˜ˆì¸¡ ì‹¤íŒ¨"

# def contains_english(user_input):
#     """ì…ë ¥ê°’ì— ì˜ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
#     return bool(re.search(r"[A-Za-z]", user_input))


# def sanitize_input(user_input):
#     """ì˜ì–´ê°€ í¬í•¨ëœ ê²½ìš° ì°¨ë‹¨"""
#     if contains_english(user_input):
#         print("âŒ ì˜ì–´ ì…ë ¥ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•œê¸€ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
#         return None
#     return user_input  # ì •ìƒ ì…ë ¥ ë°˜í™˜


def main():
    print("âœ… [ì‹œì‘] ë²•ë¥  AI ì‹¤í–‰")

    ##FAISS, BART, BERT ë¡œë“œ
    db = load_faiss()
    summarizer_tokenizer, summarizer_model = load_bart()
    load_bert()

    while True:
        user_query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
        if user_query.lower() == "exit":
            break

        # if bool(re.search(r"[A-Za-z]", user_query)):
        #     print("âŒ ì˜ì–´ ì…ë ¥ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•œê¸€ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        #     continue

        response = "ë²•ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        retrieved_text = ""

        if db:
            try:
                search_results = db.similarity_search(user_query, k=3)
                retrieved_text = "\n".join([doc.page_content for doc in search_results])
                response = retrieved_text
            except Exception as e:
                print(f"âŒ [FAISS ê²€ìƒ‰ ì˜¤ë¥˜] {e}")

        summary = "BART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"
        if summarizer_tokenizer and summarizer_model:
            summary = summarize_case(
                retrieved_text, summarizer_tokenizer, summarizer_model
            )
        # âœ… LangChainì„ í™œìš©í•œ ìµœì¢… ë‹µë³€ ìƒì„±
        final_answer = langchain_retriever.generate_legal_answer(
            user_query, summary
        )

        # âœ… ìµœì¢… ì¶œë ¥
        print("\nğŸ“Œ ê¸°ë³¸ ê²€ìƒ‰ ë‹µë³€:", response)
        print("ğŸ“Œ íŒë¡€ ìš”ì•½:", summary)
        print("\nğŸ¤– LLM ìµœì¢… ë‹µë³€:", final_answer)

if __name__ == "__main__":
    main()


# ë°ì´í„° ë½‘ê³  ë„£ì–´ì„œ ë‹¤ì‹œ ì¸¡ì • 
# ì§€ê¸ˆ ìƒí™© : ê¸°ë³¸ ë‹µë³€ì´ ìƒë‹¹íˆ ë§ì´ ë‹µë³€í•˜ê³  ë°˜ì‘ì— 5~6ì´ˆ ì •ë„ ê±¸ë¦¼ 
#  LLM ë¹„í™œì„±í™” , faiss ê²€ìƒ‰ -> bart/bert -> ì…ë ¥ 

# ê·¸ë˜ì„œ ì–´ìºë¨? faiss ì—ì„œ ê²€ìƒ‰í•´ì„œ ìœ ì‚¬í•œê²ƒì„ ë‹µí•´ì„œ ì˜ì–´ ì¶œë ¥ ë¬¸ì œë¥¼ í•´ê²°í–ˆì§€ë§Œ ì˜ˆì™¸ì²˜ë¦¬ê°€ í•„ìš” 